---
title: "Q2_Data_Science_Assessment"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, options(scipen = 999))
library(dplyr)
library(geometry)
library(ggplot2)
library(patchwork)
```


### Question 2
Let x and y be vectors of length n. Consider minimizing the loss L(b) = ||y - b x||^2 over b where b is a scalar. (The solution is b = <x, y> / ||x||^2 .) 

First, we have the loss function \[ L(b) = || y - b \cdot x ||^2 \] which is equivalently \( \sum_{i=1}^n (y_i - b \cdot x_i)^2 \)

The gradient is the derivative of \( L(b) \) with respect to \( b \): \[ \frac{dL(b)}{db} = \frac{d}{db} \left({\sum_{i=1}^n (y_i - b \cdot x_i)^2} \right) = \frac{d}{db} \left( \sum_{i=1}^n y_i^2 - 2b \sum_{i=1}^n y_i \cdot x_i + b^2 \sum_{i=1}^n x_i^2 \right)\]

We can simplify to yield the gradient to be:
\[ \frac{dL(b)}{db} = -2 \sum_{i=1}^n y_i \cdot x_i + 2b \sum_{i=1}^n x_i^2 \] 

We can check that this is correct since we are given the proper solution \(b = \frac{<x, y>} {||x||^2}\). Setting \( \frac{dL(b)}{db} = 0 \) and solving for b gives: \( \frac{\sum_{i=1}^n y_i \cdot x_i} { \sum_{i=1}^n x_i^2}= b \)

#### Q2 (a) 
Write a function in R or python that takes two vectors or numpy vectors and iterates to solve for b using gradient descent. That is, the update is: Update(b) = Current value of b - e * Derivative of L with respect to b evaluated at the current value of b

Where e is a user-supplied real number usually called the learning rate or step size.

```{r}
gradient_descent <- function(x, y, e, iterations) {
  n <- length(x) # length of vector
  b <- 0 # starting value
  for (i in 1:iterations) {
    gradient <- -2*(sum(x*y) - b*sum(x^2)) # derived above
    b <- b - e*gradient
  }
  return(b)
}

# check that b_est matches true solution
b_est = gradient_descent(x, y, e = 0.005, iterations = 5000)
b_est

# true solution
b = dot(x, y) / sum(x^2)
b
```


#### Q2 (b) 
Test your function out on some randomly generated normal vectors where you know the value of b. How does the performance of the algorithmâ€™s depend on e? When does the algorithm fail and why?

Note that we set the number of iterations to 5000, as the algorithm always converged at this specification, and that we are looking at standard normal vectors for simplicity.

The performance of the algorithm depends on the learning rate (e) such that the minimum loss is achieved in a range of e values, before which the loss is minimally higher and after which the loss exponentially increases to infinity. It appears that the range of the optimal learning rate (and the minimal loss achieved) is dependent on the details of the random vectors which were chosen. For example 2, where we generated x and y from a standard normal distribution of length 3, the well-behaved range of e is between 0.00068 and 0.17.

### Plotting function
```{r}
# Function outline
plot_fun <- function(seed, n, xlim = 0.25, title) {
  
  set.seed(seed) # set seed for reproducibility of plots
  
  # learning rates we want to plot
  e = seq(0, 0.25, by = 0.00001)
  
  x = rnorm(n) # random vectors from a standard normal distribution of length n
  y = rnorm(n)
  
  b_est = gradient_descent(x, y, e, iterations = 1000)
  
  loss = sum(y^2) - 2*b_est*dot(x, y) + (b_est^2)*sum(x^2) # expansion of original equation
  
  df = cbind(e, loss) %>% as.data.frame() 
  
  plot = df %>% ggplot(., aes(x = e, y = loss)) +
    geom_point(size = 0.5) +
    labs(title = title, 
         x = "Learning Rate (e)",
         y = expression(Loss~Function~L(hat(b)))) +
    theme_minimal() + scale_y_continuous(limits = c(0, 15)) + scale_x_continuous(limits = c(0, xlim))
  
  # return the plot
  return(plot)
}

```

```{r}
plot_fun(500, 3, 0.2, title = "Ex. 1) Length = 3") + plot_fun(55, 5, 0.23, title = "Ex. 2) Length = 5") + plot_fun(4, 10, 0.08, title = "Ex. 3) Length = 10") + plot_annotation(title = "Learning Rate vs. Loss", subtitle = "(for randomly generated standard normal vectors with varying lengths)")
```

```{r}
set.seed(500)
e = seq(0, 0.25, by = 0.00001)
n = 3
x = rnorm(n)
y = rnorm(n)
  
b_est = gradient_descent(x, y, e, iterations = 1000)
loss = sum(y^2) - 2*b_est*dot(x, y) + (b_est^2)*sum(x^2)
df = cbind(e, loss) %>% as.data.frame() 

df
```

